# LLM Provider Configuration
# Available providers: ollama, openai
DEFAULT_PROVIDER=ollama

# Ollama Configuration
OLLAMA_MODEL=qwen2.5-coder:14b
OLLAMA_MODEL_FILE_SYSTEM=qwen2.5-coder:7b
OLLAMA_BASE_URL=http://localhost:11434/v1

# OpenAI Configuration
OPENAI_MODEL=gpt-4o
OPENAI_API_KEY=your-api-key-here

# Praison configuration
OPENAI_BASE_URL=http://localhost:11434/v1 #to use Praison with Ollama


# Agent-specific Provider Overrides
CODE_CONVERTER_PROVIDER=ollama
EXPLANATION_PROVIDER=ollama
TERMINAL_PROVIDER=ollama
FILE_SYSTEM_PROVIDER=ollama
AWS_CLI_PROVIDER=ollama
DEV_ENV_PROVIDER=ollama
TERRAFORM_PROVIDER=ollama
DUCK_BROWSER_PROVIDER=ollama 
MPC_FILE_SYSTEM=ollama
MCP_SEQUENTIAL=ollama

# Disable OpenAI Telemetry (1 = disable, 0 = enable)
OPENAI_AGENTS_DISABLE_TRACING=1

# GITLAB settings
GITLAB_PERSONAL_ACCESS_TOKEN=TOKEN_HERE
GITLAB_API_URL=https://gitlab.com/api/v4

# GitHub settings
GITHUB_PERSONAL_ACCESS_TOKEN=token-here

APP_PORT=8000